#!/usr/bin/env python3
"""
Test script for Unified LLM Processor

This script tests the unified LLM processor with different providers
and demonstrates the command-line interface functionality.
"""

import asyncio
import logging
import sys
from pathlib import Path

# Add the project root to the path
sys.path.insert(0, str(Path(__file__).parent))

from rust_crate_pipeline.unified_llm_processor import (
    UnifiedLLMProcessor, 
    LLMConfig, 
    create_llm_processor_from_args
)
from rust_crate_pipeline.config import CrateMetadata


def setup_logging() -> None:
    """Setup logging configuration"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def test_llm_config_creation() -> None:
    """Test LLM configuration creation"""
    print("ðŸ§ª Testing LLM Configuration Creation...")
    
    # Test Azure OpenAI config
    azure_config = LLMConfig(
        provider="azure",
        model="gpt-4o",
        api_base="https://your-endpoint.openai.azure.com/",
        api_key="your-api-key",
        azure_deployment="gpt-4o",
        azure_api_version="2024-02-15-preview"
    )
    print(f"âœ… Azure OpenAI config created: {azure_config.provider} - {azure_config.model}")
    
    # Test Ollama config
    ollama_config = LLMConfig(
        provider="ollama",
        model="llama2",
        ollama_host="http://localhost:11434"
    )
    print(f"âœ… Ollama config created: {ollama_config.provider} - {ollama_config.model}")
    
    # Test LM Studio config
    lmstudio_config = LLMConfig(
        provider="lmstudio",
        model="llama2",
        lmstudio_host="http://localhost:1234/v1"
    )
    print(f"âœ… LM Studio config created: {lmstudio_config.provider} - {lmstudio_config.model}")
    
    # Test OpenAI config
    openai_config = LLMConfig(
        provider="openai",
        model="gpt-4",
        api_key="your-openai-key"
    )
    print(f"âœ… OpenAI config created: {openai_config.provider} - {openai_config.model}")
    
    # Test Anthropic config
    anthropic_config = LLMConfig(
        provider="anthropic",
        model="claude-3-sonnet",
        api_key="your-anthropic-key"
    )
    print(f"âœ… Anthropic config created: {anthropic_config.provider} - {anthropic_config.model}")


def test_processor_initialization() -> None:
    """Test processor initialization (without making actual API calls)"""
    print("\nðŸ§ª Testing Processor Initialization...")
    
    try:
        # Test with Azure config (will fail at API call, but should initialize)
        azure_config = LLMConfig(
            provider="azure",
            model="gpt-4o",
            api_base="https://test.openai.azure.com/",
            api_key="test-key",
            azure_deployment="gpt-4o",
            azure_api_version="2024-02-15-preview"
        )
        
        processor = UnifiedLLMProcessor(azure_config)
        print(f"âœ… Azure OpenAI processor initialized: {processor.config.provider}")
        
    except Exception as e:
        print(f"âš ï¸  Azure OpenAI processor initialization failed (expected): {e}")
    
    try:
        # Test with Ollama config
        ollama_config = LLMConfig(
            provider="ollama",
            model="llama2",
            ollama_host="http://localhost:11434"
        )
        
        processor = UnifiedLLMProcessor(ollama_config)
        print(f"âœ… Ollama processor initialized: {processor.config.provider}")
        
    except Exception as e:
        print(f"âš ï¸  Ollama processor initialization failed: {e}")


def test_crate_enrichment() -> None:
    """Test crate enrichment functionality"""
    print("\nðŸ§ª Testing Crate Enrichment...")
    
    # Create a test crate
    test_crate = CrateMetadata(
        name="test-crate",
        version="1.3.0",
        description="A test Rust crate for demonstration",
        repository="https://github.com/test/test-crate",
        keywords=["test", "demo", "rust"],
        categories=["development"],
        readme="# Test Crate\n\nThis is a test crate for demonstration purposes.\n\n## Features\n\n- Feature 1\n- Feature 2\n\n## Usage\n\n```rust\nuse test_crate;\n```",
        downloads=1000,
        github_stars=50,
        dependencies=[],
        features={},
        code_snippets=[],
        readme_sections={},
        librs_downloads=None,
        source="crates.io",
        enhanced_scraping={},
        enhanced_features=[],
        enhanced_dependencies=[]
    )
    
    print(f"âœ… Test crate created: {test_crate.name}")
    print(f"ðŸ“ Description: {test_crate.description}")
    print(f"ðŸ“š README length: {len(test_crate.readme)} characters")
    
    # Test enrichment methods (without making API calls)
    try:
        # Create a mock processor for testing
        config = LLMConfig(
            provider="ollama",
            model="llama2",
            ollama_host="http://localhost:11434"
        )
        
        processor = UnifiedLLMProcessor(config)
        
        # Test content processing methods
        truncated = processor.smart_truncate(test_crate.readme, 500)
        print(f"âœ… Content truncation test: {len(truncated)} characters")
        
        tokens = processor.estimate_tokens(test_crate.readme)
        print(f"âœ… Token estimation test: {tokens} tokens")
        
        # Test validation methods
        valid_classification = processor.validate_classification("AI")
        print(f"âœ… Classification validation test: {valid_classification}")
        
        valid_pairs = processor.validate_factual_pairs("âœ… Factual: Test\nâŒ Counterfactual: Test")
        print(f"âœ… Factual pairs validation test: {valid_pairs}")
        
    except Exception as e:
        print(f"âš ï¸  Enrichment test failed: {e}")


def test_command_line_interface() -> None:
    """Test command-line interface argument parsing"""
    print("\nðŸ§ª Testing Command-Line Interface...")
    
    # Simulate command-line arguments
    test_args = [
        "--llm-provider", "azure",
        "--llm-model", "gpt-4o",
        "--llm-api-key", "test-key",
        "--azure-deployment", "gpt-4o",
        "--crates", "tokio", "serde"
    ]
    
    print(f"ðŸ“ Test arguments: {' '.join(test_args)}")
    
    # Test argument parsing (this would normally be done by argparse)
    print("âœ… Command-line interface structure ready")
    print("ðŸ’¡ Use: python run_pipeline_with_llm.py --help for usage information")


def test_provider_support() -> None:
    """Test and display supported providers"""
    print("\nðŸ§ª Testing Provider Support...")
    
    supported_providers = [
        "azure",
        "ollama", 
        "lmstudio",
        "openai",
        "anthropic",
        "google",
        "cohere",
        "huggingface"
    ]
    
    print("ðŸ“‹ Supported LLM Providers:")
    for provider in supported_providers:
        print(f"  âœ… {provider}")
    
    print("\nðŸ”§ Provider-Specific Configuration:")
    print("  â€¢ Azure OpenAI: Requires endpoint, API key, deployment name")
    print("  â€¢ Ollama: Local server, no API key required")
    print("  â€¢ LM Studio: Local server, no API key required")
    print("  â€¢ OpenAI: Requires API key")
    print("  â€¢ Anthropic: Requires API key")
    print("  â€¢ Google AI: Requires API key")
    print("  â€¢ Cohere: Requires API key")
    print("  â€¢ Hugging Face: Requires API key")


async def main() -> None:
    """Main test function"""
    print("ðŸš€ Unified LLM Processor Test Suite")
    print("=" * 50)
    
    setup_logging()
    
    # Run tests
    test_llm_config_creation()
    test_processor_initialization()
    test_crate_enrichment()
    test_command_line_interface()
    test_provider_support()
    
    print("\n" + "=" * 50)
    print("âœ… Test suite completed!")
    print("\nðŸ“– Usage Examples:")
    print("  # Azure OpenAI")
    print("  python run_pipeline_with_llm.py --llm-provider azure --llm-model gpt-4o --crates tokio")
    print("\n  # Ollama (local)")
    print("  python run_pipeline_with_llm.py --llm-provider ollama --llm-model llama2 --crates serde")
    print("\n  # OpenAI API")
    print("  python run_pipeline_with_llm.py --llm-provider openai --llm-model gpt-4 --llm-api-key YOUR_KEY --crates tokio")
    print("\n  # Anthropic Claude")
    print("  python run_pipeline_with_llm.py --llm-provider anthropic --llm-model claude-3-sonnet --llm-api-key YOUR_KEY --crates serde")


if __name__ == "__main__":
    asyncio.run(main()) 