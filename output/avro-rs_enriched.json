{
    "name": "avro-rs",
    "version": "0.13.0",
    "description": "ALLOW: Positive community sentiment",
    "repository": "",
    "keywords": [],
    "categories": [],
    "readme": {
        "raw_markdown": "[](https://docs.rs/avro-rs/latest/avro_rs/all.html \"show sidebar\")\n# Crate avro_rsCopy item path\n[Settings](https://docs.rs/avro-rs/latest/settings.html)\n[Help](https://docs.rs/avro-rs/latest/help.html)\nSummary[Source](https://docs.rs/avro-rs/latest/src/avro_rs/lib.rs.html#1-959)\nExpand description\nA library for working with [Apache Avro](https://avro.apache.org/) in Rust.\nPlease check our [documentation](https://docs.rs/avro-rs) for examples, tutorials and API reference.\n**[Apache Avro](https://avro.apache.org/)** is a data serialization system which provides rich data structures and a compact, fast, binary data format.\nAll data in Avro is schematized, as in the following example:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"test\",\n  \"fields\": [\n    {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n    {\"name\": \"b\", \"type\": \"string\"}\n  ]\n}\n```\n\nThere are basically two ways of handling Avro data in Rust:\n  * **as Avro-specialized data types** based on an Avro schema;\n  * **as generic Rust serde-compatible types** implementing/deriving `Serialize` and `Deserialize`;\n\n\n**avro-rs** provides a way to read and write both these data representations easily and efficiently.\n## [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#installing-the-library)Installing the library\nAdd to your `Cargo.toml`:\n```\n[dependencies]\navro-rs = \"x.y\"\n```\n\nOr in case you want to leverage the **Snappy** codec:\n```\n[dependencies.avro-rs]\nversion = \"x.y\"\nfeatures = [\"snappy\"]\n```\n\n## [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#upgrading-to-a-newer-minor-version)Upgrading to a newer minor version\nThe library is still in beta, so there might be backward-incompatible changes between minor versions. If you have troubles upgrading, check the [version upgrade guide](https://docs.rs/avro-rs/latest/avro_rs/migration_guide.md).\n## [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#defining-a-schema)Defining a schema\nAn Avro data cannot exist without an Avro schema. Schemas **must** be used while writing and **can** be used while reading and they carry the information regarding the type of data we are handling. Avro schemas are used for both schema validation and resolution of Avro data.\nAvro schemas are defined in **JSON** format and can just be parsed out of a raw string:\n```\nuse avro_rs::Schema;\nlet raw_schema = r#\"\n  {\n    \"type\": \"record\",\n    \"name\": \"test\",\n    \"fields\": [\n      {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n      {\"name\": \"b\", \"type\": \"string\"}\n    ]\n  }\n\"#;\n// if the schema is not valid, this function will return an error\nlet schema = Schema::parse_str(raw_schema).unwrap();\n// schemas can be printed for debugging\nprintln!(\"{:?}\", schema);\n```\n\nAdditionally, a list of of definitions (which may depend on each other) can be given and all of them will be parsed into the corresponding schemas.\n```\nuse avro_rs::Schema;\nlet raw_schema_1 = r#\"{\n    \"name\": \"A\",\n    \"type\": \"record\",\n    \"fields\": [\n      {\"name\": \"field_one\", \"type\": \"float\"}\n    ]\n  }\"#;\n// This definition depends on the definition of A above\nlet raw_schema_2 = r#\"{\n    \"name\": \"B\",\n    \"type\": \"record\",\n    \"fields\": [\n      {\"name\": \"field_one\", \"type\": \"A\"}\n    ]\n  }\"#;\n// if the schemas are not valid, this function will return an error\nlet schemas = Schema::parse_list(&[raw_schema_1, raw_schema_2]).unwrap();\n// schemas can be printed for debugging\nprintln!(\"{:?}\", schemas);\n```\n\n_N.B._ It is important to note that the composition of schema definitions requires schemas with names. For this reason, only schemas of type Record, Enum, and Fixed should be input into this function.\nThe library provides also a programmatic interface to define schemas without encoding them in JSON (for advanced use), but we highly recommend the JSON interface. Please read the API reference in case you are interested.\nFor more information about schemas and what kind of information you can encapsulate in them, please refer to the appropriate section of the [Avro Specification](https://avro.apache.org/docs/current/spec.html#schemas).\n## [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#writing-data)Writing data\nOnce we have defined a schema, we are ready to serialize data in Avro, validating them against the provided schema in the process. As mentioned before, there are two ways of handling Avro data in Rust.\n**NOTE:** The library also provides a low-level interface for encoding a single datum in Avro bytecode without generating markers and headers (for advanced use), but we highly recommend the `Writer` interface to be totally Avro-compatible. Please read the API reference in case you are interested.\n### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#the-avro-way)The avro way\nGiven that the schema we defined above is that of an Avro _Record_ , we are going to use the associated type provided by the library to specify the data we want to serialize:\n```\nuse avro_rs::types::Record;\nuse avro_rs::Writer;\n// a writer needs a schema and something to write to\nlet mut writer = Writer::new(&schema, Vec::new());\n// the Record type models our Record schema\nlet mut record = Record::new(writer.schema()).unwrap();\nrecord.put(\"a\", 27i64);\nrecord.put(\"b\", \"foo\");\n// schema validation happens here\nwriter.append(record).unwrap();\n// this is how to get back the resulting avro bytecode\n// this performs a flush operation to make sure data has been written, so it can fail\n// you can also call `writer.flush()` yourself without consuming the writer\nlet encoded = writer.into_inner().unwrap();\n```\n\nThe vast majority of the times, schemas tend to define a record as a top-level container encapsulating all the values to convert as fields and providing documentation for them, but in case we want to directly define an Avro value, the library offers that capability via the `Value` interface.\n```\nuse avro_rs::types::Value;\nlet mut value = Value::String(\"foo\".to_string());\n```\n\n### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#the-serde-way)The serde way\nGiven that the schema we defined above is an Avro _Record_ , we can directly use a Rust struct deriving `Serialize` to model our data:\n```\nuse avro_rs::Writer;\n#[derive(Debug, Serialize)]\nstruct Test {\n  a: i64,\n  b: String,\n}\n// a writer needs a schema and something to write to\nlet mut writer = Writer::new(&schema, Vec::new());\n// the structure models our Record schema\nlet test = Test {\n  a: 27,\n  b: \"foo\".to_owned(),\n};\n// schema validation happens here\nwriter.append_ser(test).unwrap();\n// this is how to get back the resulting avro bytecode\n// this performs a flush operation to make sure data is written, so it can fail\n// you can also call `writer.flush()` yourself without consuming the writer\nlet encoded = writer.into_inner();\n```\n\nThe vast majority of the times, schemas tend to define a record as a top-level container encapsulating all the values to convert as fields and providing documentation for them, but in case we want to directly define an Avro value, any type implementing `Serialize` should work.\n```\nlet mut value = \"foo\".to_string();\n```\n\n### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#using-codecs-to-compress-data)Using codecs to compress data\nAvro supports three different compression codecs when encoding data:\n  * **Null** : leaves data uncompressed;\n  * **Deflate** : writes the data block using the deflate algorithm as specified in RFC 1951, and typically implemented using the zlib library. Note that this format (unlike the \u201czlib format\u201d in RFC 1950) does not have a checksum.\n  * **Snappy** : uses Google\u2019s [Snappy](http://google.github.io/snappy/) compression library. Each compressed block is followed by the 4-byte, big-endianCRC32 checksum of the uncompressed data in the block. You must enable the `snappy` feature to use this codec.\n\n\nTo specify a codec to use to compress data, just specify it while creating a `Writer`:\n```\nuse avro_rs::Writer;\nuse avro_rs::Codec;\nlet mut writer = Writer::with_codec(&schema, Vec::new(), Codec::Deflate);\n```\n\n## [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#reading-data)Reading data\nAs far as reading Avro encoded data goes, we can just use the schema encoded with the data to read them. The library will do it automatically for us, as it already does for the compression codec:\n```\nuse avro_rs::Reader;\n// reader creation can fail in case the input to read from is not Avro-compatible or malformed\nlet reader = Reader::new(&input[..]).unwrap();\n```\n\nIn case, instead, we want to specify a different (but compatible) reader schema from the schema the data has been written with, we can just do as the following:\n```\nuse avro_rs::Schema;\nuse avro_rs::Reader;\nlet reader_raw_schema = r#\"\n  {\n    \"type\": \"record\",\n    \"name\": \"test\",\n    \"fields\": [\n      {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n      {\"name\": \"b\", \"type\": \"string\"},\n      {\"name\": \"c\", \"type\": \"long\", \"default\": 43}\n    ]\n  }\n\"#;\nlet reader_schema = Schema::parse_str(reader_raw_schema).unwrap();\n// reader creation can fail in case the input to read from is not Avro-compatible or malformed\nlet reader = Reader::with_schema(&reader_schema, &input[..]).unwrap();\n```\n\nThe library will also automatically perform schema resolution while reading the data.\nFor more information about schema compatibility and resolution, please refer to the [Avro Specification](https://avro.apache.org/docs/current/spec.html#schemas).\nAs usual, there are two ways to handle Avro data in Rust, as you can see below.\n**NOTE:** The library also provides a low-level interface for decoding a single datum in Avro bytecode without markers and header (for advanced use), but we highly recommend the `Reader` interface to leverage all Avro features. Please read the API reference in case you are interested.\n### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#the-avro-way-1)The avro way\nWe can just read directly instances of `Value` out of the `Reader` iterator:\n```\nuse avro_rs::Reader;\nlet reader = Reader::new(&input[..]).unwrap();\n// value is a Result of an Avro Value in case the read operation fails\nfor value in reader {\n  println!(\"{:?}\", value.unwrap());\n}\n\n```\n\n### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#the-serde-way-1)The serde way\nAlternatively, we can use a Rust type implementing `Deserialize` and representing our schema to read the data into:\n```\nuse avro_rs::Reader;\nuse avro_rs::from_value;\n#[derive(Debug, Deserialize)]\nstruct Test {\n  a: i64,\n  b: String,\n}\nlet reader = Reader::new(&input[..]).unwrap();\n// value is a Result in case the read operation fails\nfor value in reader {\n  println!(\"{:?}\", from_value::<Test>(&value.unwrap()));\n}\n```\n\n## [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#putting-everything-together)Putting everything together\nThe following is an example of how to combine everything showed so far and it is meant to be a quick reference of the library interface:\n```\nuse avro_rs::{Codec, Reader, Schema, Writer, from_value, types::Record, Error};\nuse serde::{Deserialize, Serialize};\n#[derive(Debug, Deserialize, Serialize)]\nstruct Test {\n  a: i64,\n  b: String,\n}\nfn main() -> Result<(), Error> {\n  let raw_schema = r#\"\n    {\n      \"type\": \"record\",\n      \"name\": \"test\",\n      \"fields\": [\n        {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n        {\"name\": \"b\", \"type\": \"string\"}\n      ]\n    }\n  \"#;\n  let schema = Schema::parse_str(raw_schema)?;\n  println!(\"{:?}\", schema);\n  let mut writer = Writer::with_codec(&schema, Vec::new(), Codec::Deflate);\n  let mut record = Record::new(writer.schema()).unwrap();\n  record.put(\"a\", 27i64);\n  record.put(\"b\", \"foo\");\n  writer.append(record)?;\n  let test = Test {\n    a: 27,\n    b: \"foo\".to_owned(),\n  };\n  writer.append_ser(test)?;\n  let input = writer.into_inner()?;\n  let reader = Reader::with_schema(&schema, &input[..])?;\n  for record in reader {\n    println!(\"{:?}\", from_value::<Test>(&record?));\n  }\n  Ok(())\n}\n```\n\n`avro-rs` also supports the logical types listed in the [Avro specification](https://avro.apache.org/docs/current/spec.html#Logical+Types):\n  1. `Decimal` using the [`num_bigint`](https://docs.rs/num-bigint/0.2.6/num_bigint) crate\n  2. UUID using the [`uuid`](https://docs.rs/uuid/0.8.1/uuid) crate\n  3. Date, Time (milli) as `i32` and Time (micro) as `i64`\n  4. Timestamp (milli and micro) as `i64`\n  5. Duration as a custom type with `months`, `days` and `millis` accessor methods each of which returns an `i32`\n\n\nNote that the on-disk representation is identical to the underlying primitive/complex type.\n#### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#read-and-write-logical-types)Read and write logical types\n```\nuse avro_rs::{\n  types::Record, types::Value, Codec, Days, Decimal, Duration, Millis, Months, Reader, Schema,\n  Writer, Error,\n};\nuse num_bigint::ToBigInt;\nfn main() -> Result<(), Error> {\n  let raw_schema = r#\"\n  {\n   \"type\": \"record\",\n   \"name\": \"test\",\n   \"fields\": [\n    {\n     \"name\": \"decimal_fixed\",\n     \"type\": {\n      \"type\": \"fixed\",\n      \"size\": 2,\n      \"name\": \"decimal\"\n     },\n     \"logicalType\": \"decimal\",\n     \"precision\": 4,\n     \"scale\": 2\n    },\n    {\n     \"name\": \"decimal_var\",\n     \"type\": \"bytes\",\n     \"logicalType\": \"decimal\",\n     \"precision\": 10,\n     \"scale\": 3\n    },\n    {\n     \"name\": \"uuid\",\n     \"type\": \"string\",\n     \"logicalType\": \"uuid\"\n    },\n    {\n     \"name\": \"date\",\n     \"type\": \"int\",\n     \"logicalType\": \"date\"\n    },\n    {\n     \"name\": \"time_millis\",\n     \"type\": \"int\",\n     \"logicalType\": \"time-millis\"\n    },\n    {\n     \"name\": \"time_micros\",\n     \"type\": \"long\",\n     \"logicalType\": \"time-micros\"\n    },\n    {\n     \"name\": \"timestamp_millis\",\n     \"type\": \"long\",\n     \"logicalType\": \"timestamp-millis\"\n    },\n    {\n     \"name\": \"timestamp_micros\",\n     \"type\": \"long\",\n     \"logicalType\": \"timestamp-micros\"\n    },\n    {\n     \"name\": \"duration\",\n     \"type\": {\n      \"type\": \"fixed\",\n      \"size\": 12,\n      \"name\": \"duration\"\n     },\n     \"logicalType\": \"duration\"\n    }\n   ]\n  }\n  \"#;\n  let schema = Schema::parse_str(raw_schema)?;\n  println!(\"{:?}\", schema);\n  let mut writer = Writer::with_codec(&schema, Vec::new(), Codec::Deflate);\n  let mut record = Record::new(writer.schema()).unwrap();\n  record.put(\"decimal_fixed\", Decimal::from(9936.to_bigint().unwrap().to_signed_bytes_be()));\n  record.put(\"decimal_var\", Decimal::from((-32442.to_bigint().unwrap()).to_signed_bytes_be()));\n  record.put(\"uuid\", uuid::Uuid::new_v4());\n  record.put(\"date\", Value::Date(1));\n  record.put(\"time_millis\", Value::TimeMillis(2));\n  record.put(\"time_micros\", Value::TimeMicros(3));\n  record.put(\"timestamp_millis\", Value::TimestampMillis(4));\n  record.put(\"timestamp_micros\", Value::TimestampMicros(5));\n  record.put(\"duration\", Duration::new(Months::new(6), Days::new(7), Millis::new(8)));\n  writer.append(record)?;\n  let input = writer.into_inner()?;\n  let reader = Reader::with_schema(&schema, &input[..])?;\n  for record in reader {\n    println!(\"{:?}\", record?);\n  }\n  Ok(())\n}\n```\n\n### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#calculate-avro-schema-fingerprint)Calculate Avro schema fingerprint\nThis library supports calculating the following fingerprints:\n  * SHA-256\n  * MD5\n  * Rabin\n\n\nAn example of fingerprinting for the supported fingerprints:\n```\nuse avro_rs::rabin::Rabin;\nuse avro_rs::{Schema, Error};\nuse md5::Md5;\nuse sha2::Sha256;\nfn main() -> Result<(), Error> {\n  let raw_schema = r#\"\n    {\n      \"type\": \"record\",\n      \"name\": \"test\",\n      \"fields\": [\n        {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n        {\"name\": \"b\", \"type\": \"string\"}\n      ]\n    }\n  \"#;\n  let schema = Schema::parse_str(raw_schema)?;\n  println!(\"{}\", schema.fingerprint::<Sha256>());\n  println!(\"{}\", schema.fingerprint::<Md5>());\n  println!(\"{}\", schema.fingerprint::<Rabin>());\n  Ok(())\n}\n```\n\n### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#ill-formed-data)Ill-formed data\nIn order to ease decoding, the Binary Encoding specification of Avro data requires some fields to have their length encoded alongside the data.\nIf encoded data passed to a `Reader` has been ill-formed, it can happen that the bytes meant to contain the length of data are bogus and could result in extravagant memory allocation.\nTo shield users from ill-formed data, `avro-rs` sets a limit (default: 512MB) to any allocation it will perform when decoding data.\nIf you expect some of your data fields to be larger than this limit, be sure to make use of the `max_allocation_bytes` function before reading **any** data (we leverage Rust\u2019s [`std::sync::Once`](https://doc.rust-lang.org/std/sync/struct.Once.html) mechanism to initialize this value, if any call to decode is made before a call to `max_allocation_bytes`, the limit will be 512MB throughout the lifetime of the program).\n```\nuse avro_rs::max_allocation_bytes;\nmax_allocation_bytes(2 * 1024 * 1024 * 1024); // 2GB\n// ... happily decode large data\n\n```\n\n### [\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#check-schemas-compatibility)Check schemas compatibility\nThis library supports checking for schemas compatibility.\nNote: It does not yet support named schemas (more on https://github.com/flavray/avro-rs/pull/76).\nExamples of checking for compatibility:\n  1. Compatible schemas\n\n\nExplanation: an int array schema can be read by a long array schema- an int (32bit signed integer) fits into a long (64bit signed integer)\n```\nuse avro_rs::{Schema, schema_compatibility::SchemaCompatibility};\nlet writers_schema = Schema::parse_str(r#\"{\"type\": \"array\", \"items\":\"int\"}\"#).unwrap();\nlet readers_schema = Schema::parse_str(r#\"{\"type\": \"array\", \"items\":\"long\"}\"#).unwrap();\nassert_eq!(true, SchemaCompatibility::can_read(&writers_schema, &readers_schema));\n```\n\n  1. Incompatible schemas (a long array schema cannot be read by an int array schema)\n\n\nExplanation: a long array schema cannot be read by an int array schema- a long (64bit signed integer) does not fit into an int (32bit signed integer)\n```\nuse avro_rs::{Schema, schema_compatibility::SchemaCompatibility};\nlet writers_schema = Schema::parse_str(r#\"{\"type\": \"array\", \"items\":\"long\"}\"#).unwrap();\nlet readers_schema = Schema::parse_str(r#\"{\"type\": \"array\", \"items\":\"int\"}\"#).unwrap();\nassert_eq!(false, SchemaCompatibility::can_read(&writers_schema, &readers_schema));\n```\n\n## Re-exports[\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#reexports)\n\n`pub use schema::Schema[](https://docs.rs/avro-rs/latest/avro_rs/schema/enum.Schema.html \"enum avro_rs::schema::Schema\");`\n\n## Modules[\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#modules)\n\n[rabin](https://docs.rs/avro-rs/latest/avro_rs/rabin/index.html \"mod avro_rs::rabin\")\n    Implementation of the Rabin fingerprint algorithm\n\n[schema](https://docs.rs/avro-rs/latest/avro_rs/schema/index.html \"mod avro_rs::schema\")\n    Logic for parsing and interacting with schemas in Avro format.\n\n[schema_compatibility](https://docs.rs/avro-rs/latest/avro_rs/schema_compatibility/index.html \"mod avro_rs::schema_compatibility\")\n    Logic for checking schema compatibility\n\n[types](https://docs.rs/avro-rs/latest/avro_rs/types/index.html \"mod avro_rs::types\")\n    Logic handling the intermediate representation of Avro values.\n## Structs[\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#structs)\n\n[Days](https://docs.rs/avro-rs/latest/avro_rs/struct.Days.html \"struct avro_rs::Days\")\n\n\n[Decimal](https://docs.rs/avro-rs/latest/avro_rs/struct.Decimal.html \"struct avro_rs::Decimal\")\n\n\n[Duration](https://docs.rs/avro-rs/latest/avro_rs/struct.Duration.html \"struct avro_rs::Duration\")\n    A struct representing duration that hides the details of endianness and conversion between platform-native u32 and byte arrays.\n\n[Millis](https://docs.rs/avro-rs/latest/avro_rs/struct.Millis.html \"struct avro_rs::Millis\")\n\n\n[Months](https://docs.rs/avro-rs/latest/avro_rs/struct.Months.html \"struct avro_rs::Months\")\n\n\n[Reader](https://docs.rs/avro-rs/latest/avro_rs/struct.Reader.html \"struct avro_rs::Reader\")\n    Main interface for reading Avro formatted values.\n\n[Writer](https://docs.rs/avro-rs/latest/avro_rs/struct.Writer.html \"struct avro_rs::Writer\")\n    Main interface for writing Avro formatted values.\n## Enums[\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#enums)\n\n[Codec](https://docs.rs/avro-rs/latest/avro_rs/enum.Codec.html \"enum avro_rs::Codec\")\n    The compression codec used to compress blocks.\n\n[DeError](https://docs.rs/avro-rs/latest/avro_rs/enum.DeError.html \"enum avro_rs::DeError\")\n\n\n[Error](https://docs.rs/avro-rs/latest/avro_rs/enum.Error.html \"enum avro_rs::Error\")\n\n\n[SerError](https://docs.rs/avro-rs/latest/avro_rs/enum.SerError.html \"enum avro_rs::SerError\")\n\n## Functions[\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#functions)\n\n[from_avro_datum](https://docs.rs/avro-rs/latest/avro_rs/fn.from_avro_datum.html \"fn avro_rs::from_avro_datum\")\n    Decode a `Value` encoded in Avro format given its `Schema` and anything implementing `io::Read` to read from.\n\n[from_value](https://docs.rs/avro-rs/latest/avro_rs/fn.from_value.html \"fn avro_rs::from_value\")\n    Interpret a `Value` as an instance of type `D`.\n\n[max_allocation_bytes](https://docs.rs/avro-rs/latest/avro_rs/fn.max_allocation_bytes.html \"fn avro_rs::max_allocation_bytes\")\n    Set a new maximum number of bytes that can be allocated when decoding data. Once called, the limit cannot be changed.\n\n[to_avro_datum](https://docs.rs/avro-rs/latest/avro_rs/fn.to_avro_datum.html \"fn avro_rs::to_avro_datum\")\n    Encode a compatible value (implementing the `ToAvro` trait) into Avro format, also performing schema validation.\n\n[to_value](https://docs.rs/avro-rs/latest/avro_rs/fn.to_value.html \"fn avro_rs::to_value\")\n    Interpret a serializeable instance as a `Value`.\n## Type Aliases[\u00a7](https://docs.rs/avro-rs/latest/avro_rs/#types)\n\n[AvroResult](https://docs.rs/avro-rs/latest/avro_rs/type.AvroResult.html \"type avro_rs::AvroResult\")\n    A convenience type alias for `Result`s with `Error`s.\n",
        "markdown_with_citations": "[](https://docs.rs/avro-rs/latest/avro_rs/all.html \"show sidebar\")\n# Crate avro_rsCopy item path\nSettings\u27e81\u27e9\nHelp\u27e82\u27e9\nSummarySource\u27e83\u27e9\nExpand description\nA library for working with Apache Avro\u27e84\u27e9 in Rust.\nPlease check our documentation\u27e85\u27e9 for examples, tutorials and API reference.\n**Apache Avro\u27e84\u27e9** is a data serialization system which provides rich data structures and a compact, fast, binary data format.\nAll data in Avro is schematized, as in the following example:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"test\",\n  \"fields\": [\n    {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n    {\"name\": \"b\", \"type\": \"string\"}\n  ]\n}\n```\n\nThere are basically two ways of handling Avro data in Rust:\n  * **as Avro-specialized data types** based on an Avro schema;\n  * **as generic Rust serde-compatible types** implementing/deriving `Serialize` and `Deserialize`;\n\n\n**avro-rs** provides a way to read and write both these data representations easily and efficiently.\n## \u00a7\u27e86\u27e9Installing the library\nAdd to your `Cargo.toml`:\n```\n[dependencies]\navro-rs = \"x.y\"\n```\n\nOr in case you want to leverage the **Snappy** codec:\n```\n[dependencies.avro-rs]\nversion = \"x.y\"\nfeatures = [\"snappy\"]\n```\n\n## \u00a7\u27e87\u27e9Upgrading to a newer minor version\nThe library is still in beta, so there might be backward-incompatible changes between minor versions. If you have troubles upgrading, check the version upgrade guide\u27e88\u27e9.\n## \u00a7\u27e89\u27e9Defining a schema\nAn Avro data cannot exist without an Avro schema. Schemas **must** be used while writing and **can** be used while reading and they carry the information regarding the type of data we are handling. Avro schemas are used for both schema validation and resolution of Avro data.\nAvro schemas are defined in **JSON** format and can just be parsed out of a raw string:\n```\nuse avro_rs::Schema;\nlet raw_schema = r#\"\n  {\n    \"type\": \"record\",\n    \"name\": \"test\",\n    \"fields\": [\n      {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n      {\"name\": \"b\", \"type\": \"string\"}\n    ]\n  }\n\"#;\n// if the schema is not valid, this function will return an error\nlet schema = Schema::parse_str(raw_schema).unwrap();\n// schemas can be printed for debugging\nprintln!(\"{:?}\", schema);\n```\n\nAdditionally, a list of of definitions (which may depend on each other) can be given and all of them will be parsed into the corresponding schemas.\n```\nuse avro_rs::Schema;\nlet raw_schema_1 = r#\"{\n    \"name\": \"A\",\n    \"type\": \"record\",\n    \"fields\": [\n      {\"name\": \"field_one\", \"type\": \"float\"}\n    ]\n  }\"#;\n// This definition depends on the definition of A above\nlet raw_schema_2 = r#\"{\n    \"name\": \"B\",\n    \"type\": \"record\",\n    \"fields\": [\n      {\"name\": \"field_one\", \"type\": \"A\"}\n    ]\n  }\"#;\n// if the schemas are not valid, this function will return an error\nlet schemas = Schema::parse_list(&[raw_schema_1, raw_schema_2]).unwrap();\n// schemas can be printed for debugging\nprintln!(\"{:?}\", schemas);\n```\n\n_N.B._ It is important to note that the composition of schema definitions requires schemas with names. For this reason, only schemas of type Record, Enum, and Fixed should be input into this function.\nThe library provides also a programmatic interface to define schemas without encoding them in JSON (for advanced use), but we highly recommend the JSON interface. Please read the API reference in case you are interested.\nFor more information about schemas and what kind of information you can encapsulate in them, please refer to the appropriate section of the Avro Specification\u27e810\u27e9.\n## \u00a7\u27e811\u27e9Writing data\nOnce we have defined a schema, we are ready to serialize data in Avro, validating them against the provided schema in the process. As mentioned before, there are two ways of handling Avro data in Rust.\n**NOTE:** The library also provides a low-level interface for encoding a single datum in Avro bytecode without generating markers and headers (for advanced use), but we highly recommend the `Writer` interface to be totally Avro-compatible. Please read the API reference in case you are interested.\n### \u00a7\u27e812\u27e9The avro way\nGiven that the schema we defined above is that of an Avro _Record_ , we are going to use the associated type provided by the library to specify the data we want to serialize:\n```\nuse avro_rs::types::Record;\nuse avro_rs::Writer;\n// a writer needs a schema and something to write to\nlet mut writer = Writer::new(&schema, Vec::new());\n// the Record type models our Record schema\nlet mut record = Record::new(writer.schema()).unwrap();\nrecord.put(\"a\", 27i64);\nrecord.put(\"b\", \"foo\");\n// schema validation happens here\nwriter.append(record).unwrap();\n// this is how to get back the resulting avro bytecode\n// this performs a flush operation to make sure data has been written, so it can fail\n// you can also call `writer.flush()` yourself without consuming the writer\nlet encoded = writer.into_inner().unwrap();\n```\n\nThe vast majority of the times, schemas tend to define a record as a top-level container encapsulating all the values to convert as fields and providing documentation for them, but in case we want to directly define an Avro value, the library offers that capability via the `Value` interface.\n```\nuse avro_rs::types::Value;\nlet mut value = Value::String(\"foo\".to_string());\n```\n\n### \u00a7\u27e813\u27e9The serde way\nGiven that the schema we defined above is an Avro _Record_ , we can directly use a Rust struct deriving `Serialize` to model our data:\n```\nuse avro_rs::Writer;\n#[derive(Debug, Serialize)]\nstruct Test {\n  a: i64,\n  b: String,\n}\n// a writer needs a schema and something to write to\nlet mut writer = Writer::new(&schema, Vec::new());\n// the structure models our Record schema\nlet test = Test {\n  a: 27,\n  b: \"foo\".to_owned(),\n};\n// schema validation happens here\nwriter.append_ser(test).unwrap();\n// this is how to get back the resulting avro bytecode\n// this performs a flush operation to make sure data is written, so it can fail\n// you can also call `writer.flush()` yourself without consuming the writer\nlet encoded = writer.into_inner();\n```\n\nThe vast majority of the times, schemas tend to define a record as a top-level container encapsulating all the values to convert as fields and providing documentation for them, but in case we want to directly define an Avro value, any type implementing `Serialize` should work.\n```\nlet mut value = \"foo\".to_string();\n```\n\n### \u00a7\u27e814\u27e9Using codecs to compress data\nAvro supports three different compression codecs when encoding data:\n  * **Null** : leaves data uncompressed;\n  * **Deflate** : writes the data block using the deflate algorithm as specified in RFC 1951, and typically implemented using the zlib library. Note that this format (unlike the \u201czlib format\u201d in RFC 1950) does not have a checksum.\n  * **Snappy** : uses Google\u2019s Snappy\u27e815\u27e9 compression library. Each compressed block is followed by the 4-byte, big-endianCRC32 checksum of the uncompressed data in the block. You must enable the `snappy` feature to use this codec.\n\n\nTo specify a codec to use to compress data, just specify it while creating a `Writer`:\n```\nuse avro_rs::Writer;\nuse avro_rs::Codec;\nlet mut writer = Writer::with_codec(&schema, Vec::new(), Codec::Deflate);\n```\n\n## \u00a7\u27e816\u27e9Reading data\nAs far as reading Avro encoded data goes, we can just use the schema encoded with the data to read them. The library will do it automatically for us, as it already does for the compression codec:\n```\nuse avro_rs::Reader;\n// reader creation can fail in case the input to read from is not Avro-compatible or malformed\nlet reader = Reader::new(&input[..]).unwrap();\n```\n\nIn case, instead, we want to specify a different (but compatible) reader schema from the schema the data has been written with, we can just do as the following:\n```\nuse avro_rs::Schema;\nuse avro_rs::Reader;\nlet reader_raw_schema = r#\"\n  {\n    \"type\": \"record\",\n    \"name\": \"test\",\n    \"fields\": [\n      {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n      {\"name\": \"b\", \"type\": \"string\"},\n      {\"name\": \"c\", \"type\": \"long\", \"default\": 43}\n    ]\n  }\n\"#;\nlet reader_schema = Schema::parse_str(reader_raw_schema).unwrap();\n// reader creation can fail in case the input to read from is not Avro-compatible or malformed\nlet reader = Reader::with_schema(&reader_schema, &input[..]).unwrap();\n```\n\nThe library will also automatically perform schema resolution while reading the data.\nFor more information about schema compatibility and resolution, please refer to the Avro Specification\u27e810\u27e9.\nAs usual, there are two ways to handle Avro data in Rust, as you can see below.\n**NOTE:** The library also provides a low-level interface for decoding a single datum in Avro bytecode without markers and header (for advanced use), but we highly recommend the `Reader` interface to leverage all Avro features. Please read the API reference in case you are interested.\n### \u00a7\u27e817\u27e9The avro way\nWe can just read directly instances of `Value` out of the `Reader` iterator:\n```\nuse avro_rs::Reader;\nlet reader = Reader::new(&input[..]).unwrap();\n// value is a Result of an Avro Value in case the read operation fails\nfor value in reader {\n  println!(\"{:?}\", value.unwrap());\n}\n\n```\n\n### \u00a7\u27e818\u27e9The serde way\nAlternatively, we can use a Rust type implementing `Deserialize` and representing our schema to read the data into:\n```\nuse avro_rs::Reader;\nuse avro_rs::from_value;\n#[derive(Debug, Deserialize)]\nstruct Test {\n  a: i64,\n  b: String,\n}\nlet reader = Reader::new(&input[..]).unwrap();\n// value is a Result in case the read operation fails\nfor value in reader {\n  println!(\"{:?}\", from_value::<Test>(&value.unwrap()));\n}\n```\n\n## \u00a7\u27e819\u27e9Putting everything together\nThe following is an example of how to combine everything showed so far and it is meant to be a quick reference of the library interface:\n```\nuse avro_rs::{Codec, Reader, Schema, Writer, from_value, types::Record, Error};\nuse serde::{Deserialize, Serialize};\n#[derive(Debug, Deserialize, Serialize)]\nstruct Test {\n  a: i64,\n  b: String,\n}\nfn main() -> Result<(), Error> {\n  let raw_schema = r#\"\n    {\n      \"type\": \"record\",\n      \"name\": \"test\",\n      \"fields\": [\n        {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n        {\"name\": \"b\", \"type\": \"string\"}\n      ]\n    }\n  \"#;\n  let schema = Schema::parse_str(raw_schema)?;\n  println!(\"{:?}\", schema);\n  let mut writer = Writer::with_codec(&schema, Vec::new(), Codec::Deflate);\n  let mut record = Record::new(writer.schema()).unwrap();\n  record.put(\"a\", 27i64);\n  record.put(\"b\", \"foo\");\n  writer.append(record)?;\n  let test = Test {\n    a: 27,\n    b: \"foo\".to_owned(),\n  };\n  writer.append_ser(test)?;\n  let input = writer.into_inner()?;\n  let reader = Reader::with_schema(&schema, &input[..])?;\n  for record in reader {\n    println!(\"{:?}\", from_value::<Test>(&record?));\n  }\n  Ok(())\n}\n```\n\n`avro-rs` also supports the logical types listed in the Avro specification\u27e820\u27e9:\n  1. `Decimal` using the `num_bigint`\u27e821\u27e9 crate\n  2. UUID using the `uuid`\u27e822\u27e9 crate\n  3. Date, Time (milli) as `i32` and Time (micro) as `i64`\n  4. Timestamp (milli and micro) as `i64`\n  5. Duration as a custom type with `months`, `days` and `millis` accessor methods each of which returns an `i32`\n\n\nNote that the on-disk representation is identical to the underlying primitive/complex type.\n#### \u00a7\u27e823\u27e9Read and write logical types\n```\nuse avro_rs::{\n  types::Record, types::Value, Codec, Days, Decimal, Duration, Millis, Months, Reader, Schema,\n  Writer, Error,\n};\nuse num_bigint::ToBigInt;\nfn main() -> Result<(), Error> {\n  let raw_schema = r#\"\n  {\n   \"type\": \"record\",\n   \"name\": \"test\",\n   \"fields\": [\n    {\n     \"name\": \"decimal_fixed\",\n     \"type\": {\n      \"type\": \"fixed\",\n      \"size\": 2,\n      \"name\": \"decimal\"\n     },\n     \"logicalType\": \"decimal\",\n     \"precision\": 4,\n     \"scale\": 2\n    },\n    {\n     \"name\": \"decimal_var\",\n     \"type\": \"bytes\",\n     \"logicalType\": \"decimal\",\n     \"precision\": 10,\n     \"scale\": 3\n    },\n    {\n     \"name\": \"uuid\",\n     \"type\": \"string\",\n     \"logicalType\": \"uuid\"\n    },\n    {\n     \"name\": \"date\",\n     \"type\": \"int\",\n     \"logicalType\": \"date\"\n    },\n    {\n     \"name\": \"time_millis\",\n     \"type\": \"int\",\n     \"logicalType\": \"time-millis\"\n    },\n    {\n     \"name\": \"time_micros\",\n     \"type\": \"long\",\n     \"logicalType\": \"time-micros\"\n    },\n    {\n     \"name\": \"timestamp_millis\",\n     \"type\": \"long\",\n     \"logicalType\": \"timestamp-millis\"\n    },\n    {\n     \"name\": \"timestamp_micros\",\n     \"type\": \"long\",\n     \"logicalType\": \"timestamp-micros\"\n    },\n    {\n     \"name\": \"duration\",\n     \"type\": {\n      \"type\": \"fixed\",\n      \"size\": 12,\n      \"name\": \"duration\"\n     },\n     \"logicalType\": \"duration\"\n    }\n   ]\n  }\n  \"#;\n  let schema = Schema::parse_str(raw_schema)?;\n  println!(\"{:?}\", schema);\n  let mut writer = Writer::with_codec(&schema, Vec::new(), Codec::Deflate);\n  let mut record = Record::new(writer.schema()).unwrap();\n  record.put(\"decimal_fixed\", Decimal::from(9936.to_bigint().unwrap().to_signed_bytes_be()));\n  record.put(\"decimal_var\", Decimal::from((-32442.to_bigint().unwrap()).to_signed_bytes_be()));\n  record.put(\"uuid\", uuid::Uuid::new_v4());\n  record.put(\"date\", Value::Date(1));\n  record.put(\"time_millis\", Value::TimeMillis(2));\n  record.put(\"time_micros\", Value::TimeMicros(3));\n  record.put(\"timestamp_millis\", Value::TimestampMillis(4));\n  record.put(\"timestamp_micros\", Value::TimestampMicros(5));\n  record.put(\"duration\", Duration::new(Months::new(6), Days::new(7), Millis::new(8)));\n  writer.append(record)?;\n  let input = writer.into_inner()?;\n  let reader = Reader::with_schema(&schema, &input[..])?;\n  for record in reader {\n    println!(\"{:?}\", record?);\n  }\n  Ok(())\n}\n```\n\n### \u00a7\u27e824\u27e9Calculate Avro schema fingerprint\nThis library supports calculating the following fingerprints:\n  * SHA-256\n  * MD5\n  * Rabin\n\n\nAn example of fingerprinting for the supported fingerprints:\n```\nuse avro_rs::rabin::Rabin;\nuse avro_rs::{Schema, Error};\nuse md5::Md5;\nuse sha2::Sha256;\nfn main() -> Result<(), Error> {\n  let raw_schema = r#\"\n    {\n      \"type\": \"record\",\n      \"name\": \"test\",\n      \"fields\": [\n        {\"name\": \"a\", \"type\": \"long\", \"default\": 42},\n        {\"name\": \"b\", \"type\": \"string\"}\n      ]\n    }\n  \"#;\n  let schema = Schema::parse_str(raw_schema)?;\n  println!(\"{}\", schema.fingerprint::<Sha256>());\n  println!(\"{}\", schema.fingerprint::<Md5>());\n  println!(\"{}\", schema.fingerprint::<Rabin>());\n  Ok(())\n}\n```\n\n### \u00a7\u27e825\u27e9Ill-formed data\nIn order to ease decoding, the Binary Encoding specification of Avro data requires some fields to have their length encoded alongside the data.\nIf encoded data passed to a `Reader` has been ill-formed, it can happen that the bytes meant to contain the length of data are bogus and could result in extravagant memory allocation.\nTo shield users from ill-formed data, `avro-rs` sets a limit (default: 512MB) to any allocation it will perform when decoding data.\nIf you expect some of your data fields to be larger than this limit, be sure to make use of the `max_allocation_bytes` function before reading **any** data (we leverage Rust\u2019s `std::sync::Once`\u27e826\u27e9 mechanism to initialize this value, if any call to decode is made before a call to `max_allocation_bytes`, the limit will be 512MB throughout the lifetime of the program).\n```\nuse avro_rs::max_allocation_bytes;\nmax_allocation_bytes(2 * 1024 * 1024 * 1024); // 2GB\n// ... happily decode large data\n\n```\n\n### \u00a7\u27e827\u27e9Check schemas compatibility\nThis library supports checking for schemas compatibility.\nNote: It does not yet support named schemas (more on https://github.com/flavray/avro-rs/pull/76).\nExamples of checking for compatibility:\n  1. Compatible schemas\n\n\nExplanation: an int array schema can be read by a long array schema- an int (32bit signed integer) fits into a long (64bit signed integer)\n```\nuse avro_rs::{Schema, schema_compatibility::SchemaCompatibility};\nlet writers_schema = Schema::parse_str(r#\"{\"type\": \"array\", \"items\":\"int\"}\"#).unwrap();\nlet readers_schema = Schema::parse_str(r#\"{\"type\": \"array\", \"items\":\"long\"}\"#).unwrap();\nassert_eq!(true, SchemaCompatibility::can_read(&writers_schema, &readers_schema));\n```\n\n  1. Incompatible schemas (a long array schema cannot be read by an int array schema)\n\n\nExplanation: a long array schema cannot be read by an int array schema- a long (64bit signed integer) does not fit into an int (32bit signed integer)\n```\nuse avro_rs::{Schema, schema_compatibility::SchemaCompatibility};\nlet writers_schema = Schema::parse_str(r#\"{\"type\": \"array\", \"items\":\"long\"}\"#).unwrap();\nlet readers_schema = Schema::parse_str(r#\"{\"type\": \"array\", \"items\":\"int\"}\"#).unwrap();\nassert_eq!(false, SchemaCompatibility::can_read(&writers_schema, &readers_schema));\n```\n\n## Re-exports\u00a7\u27e828\u27e9\n\n`pub use schema::Schema[](https://docs.rs/avro-rs/latest/avro_rs/schema/enum.Schema.html \"enum avro_rs::schema::Schema\");`\n\n## Modules\u00a7\u27e829\u27e9\n\nrabin\u27e830\u27e9\n    Implementation of the Rabin fingerprint algorithm\n\nschema\u27e831\u27e9\n    Logic for parsing and interacting with schemas in Avro format.\n\nschema_compatibility\u27e832\u27e9\n    Logic for checking schema compatibility\n\ntypes\u27e833\u27e9\n    Logic handling the intermediate representation of Avro values.\n## Structs\u00a7\u27e834\u27e9\n\nDays\u27e835\u27e9\n\n\nDecimal\u27e836\u27e9\n\n\nDuration\u27e837\u27e9\n    A struct representing duration that hides the details of endianness and conversion between platform-native u32 and byte arrays.\n\nMillis\u27e838\u27e9\n\n\nMonths\u27e839\u27e9\n\n\nReader\u27e840\u27e9\n    Main interface for reading Avro formatted values.\n\nWriter\u27e841\u27e9\n    Main interface for writing Avro formatted values.\n## Enums\u00a7\u27e842\u27e9\n\nCodec\u27e843\u27e9\n    The compression codec used to compress blocks.\n\nDeError\u27e844\u27e9\n\n\nError\u27e845\u27e9\n\n\nSerError\u27e846\u27e9\n\n## Functions\u00a7\u27e847\u27e9\n\nfrom_avro_datum\u27e848\u27e9\n    Decode a `Value` encoded in Avro format given its `Schema` and anything implementing `io::Read` to read from.\n\nfrom_value\u27e849\u27e9\n    Interpret a `Value` as an instance of type `D`.\n\nmax_allocation_bytes\u27e850\u27e9\n    Set a new maximum number of bytes that can be allocated when decoding data. Once called, the limit cannot be changed.\n\nto_avro_datum\u27e851\u27e9\n    Encode a compatible value (implementing the `ToAvro` trait) into Avro format, also performing schema validation.\n\nto_value\u27e852\u27e9\n    Interpret a serializeable instance as a `Value`.\n## Type Aliases\u00a7\u27e853\u27e9\n\nAvroResult\u27e854\u27e9\n    A convenience type alias for `Result`s with `Error`s.\n",
        "references_markdown": "\n\n## References\n\n\u27e81\u27e9 https://docs.rs/avro-rs/latest/settings.html: Settings\n\u27e82\u27e9 https://docs.rs/avro-rs/latest/help.html: Help\n\u27e83\u27e9 https://docs.rs/avro-rs/latest/src/avro_rs/lib.rs.html#1-959: Source\n\u27e84\u27e9 https://avro.apache.org/: Apache Avro\n\u27e85\u27e9 https://docs.rs/avro-rs: documentation\n\u27e86\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#installing-the-library: \u00a7\n\u27e87\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#upgrading-to-a-newer-minor-version: \u00a7\n\u27e88\u27e9 https://docs.rs/avro-rs/latest/avro_rs/migration_guide.md: version upgrade guide\n\u27e89\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#defining-a-schema: \u00a7\n\u27e810\u27e9 https://avro.apache.org/docs/current/spec.html#schemas: Avro Specification\n\u27e811\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#writing-data: \u00a7\n\u27e812\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#the-avro-way: \u00a7\n\u27e813\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#the-serde-way: \u00a7\n\u27e814\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#using-codecs-to-compress-data: \u00a7\n\u27e815\u27e9 http://google.github.io/snappy/: Snappy\n\u27e816\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#reading-data: \u00a7\n\u27e817\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#the-avro-way-1: \u00a7\n\u27e818\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#the-serde-way-1: \u00a7\n\u27e819\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#putting-everything-together: \u00a7\n\u27e820\u27e9 https://avro.apache.org/docs/current/spec.html#Logical+Types: Avro specification\n\u27e821\u27e9 https://docs.rs/num-bigint/0.2.6/num_bigint: `num_bigint`\n\u27e822\u27e9 https://docs.rs/uuid/0.8.1/uuid: `uuid`\n\u27e823\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#read-and-write-logical-types: \u00a7\n\u27e824\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#calculate-avro-schema-fingerprint: \u00a7\n\u27e825\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#ill-formed-data: \u00a7\n\u27e826\u27e9 https://doc.rust-lang.org/std/sync/struct.Once.html: `std::sync::Once`\n\u27e827\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#check-schemas-compatibility: \u00a7\n\u27e828\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#reexports: \u00a7\n\u27e829\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#modules: \u00a7\n\u27e830\u27e9 https://docs.rs/avro-rs/latest/avro_rs/rabin/index.html: mod avro_rs::rabin - rabin\n\u27e831\u27e9 https://docs.rs/avro-rs/latest/avro_rs/schema/index.html: mod avro_rs::schema - schema\n\u27e832\u27e9 https://docs.rs/avro-rs/latest/avro_rs/schema_compatibility/index.html: mod avro_rs::schema_compatibility - schema_compatibility\n\u27e833\u27e9 https://docs.rs/avro-rs/latest/avro_rs/types/index.html: mod avro_rs::types - types\n\u27e834\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#structs: \u00a7\n\u27e835\u27e9 https://docs.rs/avro-rs/latest/avro_rs/struct.Days.html: struct avro_rs::Days - Days\n\u27e836\u27e9 https://docs.rs/avro-rs/latest/avro_rs/struct.Decimal.html: struct avro_rs::Decimal - Decimal\n\u27e837\u27e9 https://docs.rs/avro-rs/latest/avro_rs/struct.Duration.html: struct avro_rs::Duration - Duration\n\u27e838\u27e9 https://docs.rs/avro-rs/latest/avro_rs/struct.Millis.html: struct avro_rs::Millis - Millis\n\u27e839\u27e9 https://docs.rs/avro-rs/latest/avro_rs/struct.Months.html: struct avro_rs::Months - Months\n\u27e840\u27e9 https://docs.rs/avro-rs/latest/avro_rs/struct.Reader.html: struct avro_rs::Reader - Reader\n\u27e841\u27e9 https://docs.rs/avro-rs/latest/avro_rs/struct.Writer.html: struct avro_rs::Writer - Writer\n\u27e842\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#enums: \u00a7\n\u27e843\u27e9 https://docs.rs/avro-rs/latest/avro_rs/enum.Codec.html: enum avro_rs::Codec - Codec\n\u27e844\u27e9 https://docs.rs/avro-rs/latest/avro_rs/enum.DeError.html: enum avro_rs::DeError - DeError\n\u27e845\u27e9 https://docs.rs/avro-rs/latest/avro_rs/enum.Error.html: enum avro_rs::Error - Error\n\u27e846\u27e9 https://docs.rs/avro-rs/latest/avro_rs/enum.SerError.html: enum avro_rs::SerError - SerError\n\u27e847\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#functions: \u00a7\n\u27e848\u27e9 https://docs.rs/avro-rs/latest/avro_rs/fn.from_avro_datum.html: fn avro_rs::from_avro_datum - from_avro_datum\n\u27e849\u27e9 https://docs.rs/avro-rs/latest/avro_rs/fn.from_value.html: fn avro_rs::from_value - from_value\n\u27e850\u27e9 https://docs.rs/avro-rs/latest/avro_rs/fn.max_allocation_bytes.html: fn avro_rs::max_allocation_bytes - max_allocation_bytes\n\u27e851\u27e9 https://docs.rs/avro-rs/latest/avro_rs/fn.to_avro_datum.html: fn avro_rs::to_avro_datum - to_avro_datum\n\u27e852\u27e9 https://docs.rs/avro-rs/latest/avro_rs/fn.to_value.html: fn avro_rs::to_value - to_value\n\u27e853\u27e9 https://docs.rs/avro-rs/latest/avro_rs/#types: \u00a7\n\u27e854\u27e9 https://docs.rs/avro-rs/latest/avro_rs/type.AvroResult.html: type avro_rs::AvroResult - AvroResult\n",
        "fit_markdown": "",
        "fit_html": ""
    },
    "downloads": 0,
    "github_stars": 0,
    "dependencies": [],
    "features": {},
    "code_snippets": [],
    "readme_sections": {},
    "librs_downloads": null,
    "source": "crates.io",
    "enhanced_scraping": {},
    "enhanced_features": [],
    "enhanced_dependencies": [],
    "readme_summary": "The `avro-rs` crate provides a Rust library for working with Apache Avro, a compact and fast data serialization system. It supports reading and writing Avro data using JSON-defined schemas or Rust's `serde` traits, schema validation and resolution, logical types (e.g., UUID, Decimal, Date/Time), and compression codecs like Deflate and Snappy. Additional features include schema compatibility checks, fingerprinting, and safeguards against ill-formed data.",
    "feature_summary": null,
    "use_case": "Serialization",
    "score": 6.0,
    "factual_counterfactual": "### Pair 1: Schema Parsing  \n\u2705 **Factual**: The `avro-rs` crate supports parsing Avro schemas defined in JSON format using the `Schema::parse_str` method, which returns an error if the schema is invalid.  \n\u274c **Counterfactual**: The `avro-rs` crate can parse Avro schemas directly from YAML format without requiring any conversion to JSON.\n\n---\n\n### Pair 2: Compression Codecs  \n\u2705 **Factual**: The `avro-rs` crate supports three compression codecs for encoding data: `Null`, `Deflate`, and `Snappy`. The `Snappy` codec requires enabling the `snappy` feature in `Cargo.toml`.  \n\u274c **Counterfactual**: The `avro-rs` crate supports additional compression codecs such as `LZ4` and `Zstandard` for encoding data.\n\n---\n\n### Pair 3: Logical Types  \n\u2705 **Factual**: The `avro-rs` crate supports logical types like `Decimal`, `UUID`, and `Timestamp`, with on-disk representations identical to their underlying primitive types.  \n\u274c **Counterfactual**: The `avro-rs` crate does not support logical types and requires users to manually handle conversions for complex data types like `Decimal` or `UUID`.  \n\n---\n\n### Pair 4: Serde Integration  \n\u2705 **Factual**: The `avro-rs` crate",
    "source_analysis": null,
    "user_behavior": null,
    "security": null
}